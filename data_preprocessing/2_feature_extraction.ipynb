{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cite from Xinyi6\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890fc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/us_financial_news_articles_2018.csv', index_col=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_percentage = df.isnull().sum()/df.shape[0] * 100\n",
    "df_missing_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('missing')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c877d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# TF-IDF: term frequencyâ€“inverse document frequency\n",
    "# Bag of Words\n",
    "def create_tfidf(df, feature_column, max_feature_size):\n",
    "    from sklearn.feature_extraction import text\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union([\"ap1\", \"00\", \"000\", \"0\", \"561\", \"190\", \"09\", \"24\", \"2017\", \"2018\", \"000 00\", \"2018\",\n",
    "                                                   \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\",\n",
    "                                                   \"ag\", \"ap3\", \"000 00 ap3\", \"ap2\", \"00 ap2\", \"000 00 ap2\", \"10\", \"00 ap1\",\n",
    "                                                   \"000 00 ap1\", \"oct 2018\", \"000 000\", \"000 000 00\", \"october 2018\", \"10 2018\",\n",
    "                                                   \"11 2018\", \"november 2018\", \"12 2018\", \"december 2018\"\n",
    "\n",
    "\n",
    "                                                   ])\n",
    "    # my_stop_words=my_stop_words.difference([\"call\"])\n",
    "\n",
    "    # initilize\n",
    "    tfidf_vec = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding=\"latin-1\",\n",
    "                                ngram_range=(1, 3), stop_words=my_stop_words, max_features=max_feature_size)\n",
    "    features = tfidf_vec.fit_transform(df[feature_column]).toarray()\n",
    "    return features, tfidf_vec\n",
    "\n",
    "\n",
    "def create_bow(df, feature_column, max_feature_size):\n",
    "    from sklearn.feature_extraction import text\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union([\"ap1\", \"00\", \"000\", \"0\", \"561\", \"190\", \"09\", \"24\", \"2017\", \"2018\", \"000 00\", \"2018\",\n",
    "                                                   \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\",\n",
    "                                                   \"ag\", \"ap3\", \"000 00 ap3\", \"ap2\", \"00 ap2\", \"000 00 ap2\", \"10\", \"00 ap1\",\n",
    "                                                   \"000 00 ap1\", \"oct 2018\", \"000 000\", \"000 000 00\", \"october 2018\", \"10 2018\",\n",
    "                                                   \"11 2018\", \"november 2018\", \"12 2018\", \"december 2018\"\n",
    "                                                   ])\n",
    "    # my_stop_words=my_stop_words.difference([\"call\"])\n",
    "\n",
    "    # initilize\n",
    "    Counter_vec = CountVectorizer(encoding=\"latin-1\", ngram_range=(\n",
    "        1, 3), stop_words=my_stop_words, max_features=max_feature_size)\n",
    "    features = Counter_vec.fit_transform(df[feature_column]).toarray()\n",
    "    return features, Counter_vec\n",
    "\n",
    "\n",
    "def create_words_frequency(features, features_name):\n",
    "    features_df = pd.DataFrame(features)\n",
    "    features_df.columns = features_name\n",
    "    sorted_features = features_df.sum(axis=0).sort_values(ascending=False)\n",
    "    sorted_features = pd.DataFrame(sorted_features)\n",
    "    sorted_features = sorted_features.reset_index()\n",
    "    sorted_features.columns = ['Top Words', 'Counts']\n",
    "    return sorted_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tfidf, tfidf_vec = create_tfidf(\n",
    "    df, feature_column='title', max_feature_size=5000)\n",
    "features_tfidf_names = tfidf_vec.get_feature_names()\n",
    "\n",
    "tfidf_sorted_table = create_words_frequency(\n",
    "    features_tfidf, features_tfidf_names)\n",
    "tfidf_sorted_table.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bow, bow_vec = create_bow(\n",
    "    df, feature_column='title', max_feature_size=5000)\n",
    "features_bow_names = bow_vec.get_feature_names()\n",
    "bow_sorted_table = create_words_frequency(features_bow, features_bow_names)\n",
    "bow_sorted_table.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Score\n",
    "# SentimentIntensityAnalyzer package\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_score = [sid.polarity_scores(sent) for sent in df.title]\n",
    "len(title_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a343db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title[0:10].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013be7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_score[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a19380",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound = []\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "\n",
    "for i in range(len(title_score)):\n",
    "    compound.append(title_score[i]['compound'])\n",
    "    neg.append(title_score[i]['neg'])\n",
    "    neu.append(title_score[i]['neu'])\n",
    "    pos.append(title_score[i]['pos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3221848",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(compound)\n",
    "len(neg)\n",
    "len(neu)\n",
    "len(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compound'] = compound\n",
    "df['neg'] = neg\n",
    "df['neu'] = neu\n",
    "df['pos'] = pos\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to .csv file\n",
    "# df.to_csv('../data/us_financial_news_articles_2018_with_sentiment.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "30855fca1de1a9e76fb191234c06e457444d085aee49bba65a45b68036adabaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
